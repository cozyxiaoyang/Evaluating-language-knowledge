{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.__version__: 4.17.0\n",
      "tokenizers.__version__: 0.11.6\n"
     ]
    }
   ],
   "source": [
    "import os,gc,re,ast,sys,copy,json,time,datetime,math,string,pickle,random,joblib,itertools\n",
    "\n",
    "from distutils.util import strtobool\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold,train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "import transformers,tokenizers\n",
    "print(f'transformers.__version__: {transformers.__version__}')\n",
    "print(f'tokenizers.__version__: {tokenizers.__version__}')\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "os.environ['TOKENIZERS_PARALLELISM']='true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./microsoft-deberta-v3-base/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    model_name = \"microsoft/deberta-v3-base\"\n",
    "    model_path = \"D:/deberta-v3-base\"\n",
    "    \n",
    "    \n",
    "    batch_size ,n_targets,num_workers = 8,6,4\n",
    "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    epochs,print_freq = 5,20 # 训练时每隔20step打印一次    \n",
    "    save_all_models=False # 是否每个epoch都保存数据\n",
    "    gradient_checkpointing = True\n",
    "    \n",
    "    loss_func = 'SmoothL1' # 'SmoothL1', 'RMSE'\n",
    "    pooling = 'attention' # mean, max, min, attention, weightedlayer\n",
    "    gradient_checkpointing = True\n",
    "    gradient_accumulation_steps = 1 # 是否使用梯度累积更新\n",
    "    max_grad_norm = 1000 #梯度裁剪\n",
    "    apex = True # 是否进行自动混合精度训练 \n",
    "    \n",
    "    # 启用llrd\n",
    "    layerwise_lr,layerwise_lr_decay = 5e-5,0.9\n",
    "    layerwise_weight_decay = 0.01\n",
    "    layerwise_adam_epsilon = 1e-6\n",
    "    layerwise_use_bertadam = False\n",
    "    \n",
    "    scheduler = 'cosine'\n",
    "    num_cycles ,num_warmup_steps= 0.5,0\n",
    "    encoder_lr,decoder_lr,min_lr  = 2e-5,2e-5 ,1e-6\n",
    "    max_len = 512\n",
    "    weight_decay = 0.01\n",
    "    \n",
    "    fgm = True # 是否使用fgm对抗网络攻击\n",
    "    wandb=False\n",
    "    adv_lr,adv_eps,eps,betas = 1,0.2,1e-6,(0.9, 0.999)\n",
    "    unscale =True\n",
    "    \n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=list(range(n_fold))\n",
    "    debug=False # debug表示只使用少量样本跑代码，且n_fold=2，epoch=2\n",
    "    \n",
    "    OUTPUT_DIR = f\"./{model_name.replace('/', '-')}/\"\n",
    "    train_file = 'C:/Users/86134/Desktop/train.csv'\n",
    "    test_file = 'C:/Users/86134/Desktop/test.csv'\n",
    "    submission_file = 'C:/Users/86134/Desktop/sample_submission.csv'\n",
    "    \n",
    "if not os.path.exists(CFG.OUTPUT_DIR):\n",
    "    os.makedirs(CFG.OUTPUT_DIR)\n",
    "    \n",
    "CFG.OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子 固定结果\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)  # 保证后续使用random函数时，产生固定的随机数\n",
    "    torch.manual_seed(seed)  # 固定随机种子（CPU）\n",
    "    if torch.cuda.is_available():  # 固定随机种子（GPU)\n",
    "        torch.cuda.manual_seed(seed)  # 为当前GPU设置\n",
    "        torch.cuda.manual_seed_all(seed)  # 为所有GPU设置\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True  # 固定网络结构\n",
    "    \n",
    "set_seeds(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.wandb:    \n",
    "    import wandb\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        api_key = user_secrets.get_secret(\"WANDB\")\n",
    "        wandb.login(key=api_key)\n",
    "    except:\n",
    "        wandb.login(anonymous='must')\n",
    "        print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "    def class2dict(f):\n",
    "        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "    run = wandb.init(project='FB3-Public', \n",
    "                     name=CFG.model,\n",
    "                     config=class2dict(CFG),\n",
    "                     group=CFG.model,\n",
    "                     job_type=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[   1,  335,  266, ...,  265,  262,    2],\n",
       "       [   1,  771,  274, ...,    0,    0,    0],\n",
       "       [   1, 2651, 9805, ...,    0,    0,    0]]), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "def preprocess(df,tokenizer,types=True):\n",
    "    if types:\n",
    "        labels = np.array(df[[\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]])\n",
    "    else:\n",
    "        labels=df[\"labels\"]\n",
    "    text=list(df['full_text'].iloc[:])\n",
    "    encoding=tokenizer(text,truncation=True,padding=\"max_length\",\n",
    "                        max_length=CFG.max_len,return_tensors=\"np\")#训练集中划分的训练集\n",
    "    return encoding,labels\n",
    "    \n",
    "\n",
    "df=pd.read_csv(CFG.train_file)\n",
    "test_df = pd.read_csv(CFG.test_file)\n",
    "test_df['labels']=None\n",
    "test_df['labels']=test_df['labels'].apply(lambda x:[0,0,0,0,0,0])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n",
    "test_encoding,test_label=preprocess(test_df,tokenizer,False)\n",
    "test_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 876.6 kB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载到datalodar并预处理\n",
    "#数据集读取\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,encoding,label):\n",
    "        self.inputs=encoding\n",
    "        self.label=label\n",
    "        \n",
    "\n",
    "    # 读取单个样本\n",
    "    def __getitem__(self,idx):\n",
    "        item={key:torch.tensor(val[idx],dtype = torch.long) for key,val in self.inputs.items()}\n",
    "        label=torch.tensor(self.label[idx],dtype=torch.float)\n",
    "        return item,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "def collate(inputs): \n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs\n",
    "\n",
    "test_dataset=MyDataset(test_encoding,test_label)\n",
    "test_loader=DataLoader(test_dataset,batch_size=CFG.batch_size,\n",
    "                       num_workers=CFG.num_workers,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger __main__ (INFO)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction = 'mean', eps = 1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction = 'none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss  \n",
    "\n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:, i]\n",
    "        y_pred = y_preds[:, i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared = False)\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "   \n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{int(m)}m {int(s)}s'\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return f'{str(asMinutes(s))} (remain {str(asMinutes(rs))})' \n",
    "\n",
    "def get_logger(filename=CFG.OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "if CFG.loss_func == 'SmoothL1':\n",
    "    criterion = nn.SmoothL1Loss(reduction='mean')\n",
    "elif CFG.loss_func == 'RMSE':\n",
    "    criterion = RMSELoss(reduction='mean')\n",
    "    \n",
    "logger= get_logger()\n",
    "logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon = 1., emb_name = 'word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name = 'word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  fold  \n",
       "0     3.5         3.0          3.0      4.0          3.0     2  \n",
       "1     2.5         3.0          2.0      2.0          2.5     0  \n",
       "2     3.5         3.0          3.0      3.0          2.5     1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append('D:/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "Fold = MultilabelStratifiedKFold(n_splits = CFG.n_fold, shuffle = True, random_state = CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(df, df[CFG.target_cols])):\n",
    "    df.loc[val_index, 'fold'] = int(n)\n",
    "df['fold'] = df['fold'].astype(int)\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0,1]\n",
    "    df = df.sample(n = 100, random_state = CFG.seed).reset_index(drop=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min = 1e-9)\n",
    "        mean_embeddings = sum_embeddings/sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = -1e4\n",
    "        max_embeddings, _ = torch.max(embeddings, dim = 1)\n",
    "        return max_embeddings\n",
    "    \n",
    "class MinPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = 1e-4\n",
    "        min_embeddings, _ = torch.min(embeddings, dim = 1)\n",
    "        return min_embeddings\n",
    "\n",
    "#Attention pooling\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "        nn.Linear(in_dim, in_dim),\n",
    "        nn.LayerNorm(in_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(in_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask==0]=float('-inf')\n",
    "        w = torch.softmax(w,1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings\n",
    "\n",
    "#There may be a bug in my implementation because it does not work well.\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, ft_all_layers):\n",
    "        all_layer_embedding = torch.stack(ft_all_layers)\n",
    "        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n",
    "\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FB3Model(nn.Module):\n",
    "    def __init__(self, CFG, config_path = None,pretrained=False):\n",
    "        super().__init__()\n",
    "        self.CFG = CFG\n",
    "        # 设置模型的config文件，根据此配置文件读取预训练模型\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(CFG.model_path, ouput_hidden_states = True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.            \n",
    "            \n",
    "        else:\n",
    "            self.config = torch.load(config_path)   \n",
    "        #logger.info(self.config)\n",
    "        \n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(CFG.model_path, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "       \n",
    "            \n",
    "        if CFG.pooling == 'mean':\n",
    "            self.pool = MeanPooling()\n",
    "        elif CFG.pooling == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        elif CFG.pooling == 'min':\n",
    "            self.pool = MinPooling()\n",
    "        elif CFG.pooling == 'attention':\n",
    "            self.pool = AttentionPooling(self.config.hidden_size)\n",
    "        elif CFG.pooling == 'weightedlayer':\n",
    "            self.pool = WeightedLayerPooling(self.config.num_hidden_layers, layer_start = CFG.layer_start, layer_weights = None)        \n",
    "        # 用一个全连接层得到预测的6类输出\n",
    "        self.fc = nn.Linear(self.config.hidden_size, self.CFG.n_targets)\n",
    "   \n",
    "   # 根据池化方法选择输出\n",
    "    def feature(self,inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        if CFG.pooling != 'weightedlayer':\n",
    "            last_hidden_states = outputs[0]\n",
    "            feature = self.pool(last_hidden_states,inputs['attention_mask'])\n",
    "        else:\n",
    "            all_layer_embeddings = outputs[1]\n",
    "            feature = self.pool(all_layer_embeddings)\n",
    "            \n",
    "        return feature\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        outout = self.fc(feature)\n",
    "        return outout     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLDR\n",
    "def get_optimizer_grouped_parameters(model, layerwise_lr,layerwise_weight_decay,layerwise_lr_decay):\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    # initialize lr for task specific layer\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "                                     \"weight_decay\": 0.0,\"lr\": layerwise_lr,},]\n",
    "    # initialize lrs for every layer\n",
    "    layers = [model.model.embeddings] + list(model.model.encoder.layer)\n",
    "    layers.reverse()\n",
    "    lr = layerwise_lr\n",
    "    for layer in layers:\n",
    "        optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                                          \"weight_decay\": layerwise_weight_decay,\"lr\": lr,},\n",
    "                                         {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                          \"weight_decay\": 0.0,\"lr\": lr,},]\n",
    "        lr *= layerwise_lr_decay\n",
    "    return optimizer_grouped_parameters\n",
    "                \n",
    "    \n",
    "# 选择使用线性学习率衰减或者cos学习率衰减\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps = cfg.num_warmup_steps, \n",
    "            num_training_steps = num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps = cfg.num_warmup_steps, \n",
    "            num_training_steps = num_train_steps,\n",
    "            num_cycles = cfg.num_cycles\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold,train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = CFG.apex) # 自动混合精度训练\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    if CFG.fgm:\n",
    "        fgm = FGM(model) # 对抗训练\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        #attention_mask = inputs['attention_mask'].to(device)\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled = CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        if CFG.unscale:\n",
    "            scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        \n",
    "        #Fast Gradient Method (FGM)\n",
    "        if CFG.fgm:\n",
    "            fgm.attack()\n",
    "            with torch.cuda.amp.autocast(enabled = CFG.apex):\n",
    "                y_preds = model(inputs)\n",
    "                loss_adv = criterion(y_preds, labels)\n",
    "                loss_adv.backward()\n",
    "            fgm.restore()\n",
    "            \n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            scheduler.step()\n",
    "        end = time.time()\n",
    "        \n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f} '\n",
    "                  'LR: {lr:.8f} '\n",
    "                  .format(epoch + 1, step, len(train_loader), remain = timeSince(start, float(step + 1)/len(train_loader)),\n",
    "                          loss = losses,\n",
    "                          grad_norm = grad_norm,\n",
    "                          lr = scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({\" loss\": losses.val,\n",
    "                       \" lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train_loop():                   \n",
    "    best_score = np.inf   \n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"========== epoch: {epoch} training ==========\")\n",
    "\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, CFG.device)\n",
    "        avg_val_loss, predictions,valid_labels = valid_fn(val_loader, model, criterion, CFG.device)\n",
    "        \n",
    "        score, scores = MCRMSE(valid_labels, predictions)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        logger.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        logger.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        \n",
    "        if CFG.wandb:\n",
    "            wandb.log({\" epoch\": epoch+1, \n",
    "                       \" avg_train_loss\": avg_loss, \n",
    "                       \" avg_val_loss\": avg_val_loss,\n",
    "                       \" score\": score})                  \n",
    "        \n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            logger.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        CFG.OUTPUT_DIR + \"_best.pth\")\n",
    "            \n",
    "        if CFG.save_all_models:\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        CFG.OUTPUT_DIR + \"_epoch{epoch + 1}.pth\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(df, fold):\n",
    "    logger.info(f\"========== fold: {fold} training ==========\")\n",
    "    # 加载数据集\n",
    "    train_folds = df[df['fold'] != fold].reset_index(drop = True)\n",
    "    valid_folds = df[df['fold'] == fold].reset_index(drop = True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_encoding,train_label=preprocess(train_folds,tokenizer,True)\n",
    "    val_encoding,val_label=preprocess(valid_folds,tokenizer,True)\n",
    "    \n",
    "    train_dataset = MyDataset(train_encoding,train_label)\n",
    "    valid_dataset = MyDataset(val_encoding,val_label)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset,batch_size = CFG.batch_size,shuffle = True, \n",
    "                              num_workers = CFG.num_workers,pin_memory = True)\n",
    "    valid_loader = DataLoader(valid_dataset,batch_size = CFG.batch_size * 2,\n",
    "                              shuffle=False,num_workers=CFG.num_workers,pin_memory=True, )\n",
    "    \n",
    "    model = FB3Model(CFG, config_path = None,pretrained=True) \n",
    "    torch.save(model.config, CFG.OUTPUT_DIR +'./config.pth')\n",
    "    model.to(CFG.device)  \n",
    "    # 加载优化器和调度器\n",
    "    from torch.optim import AdamW\n",
    "    grouped_optimizer_params = get_optimizer_grouped_parameters(model, \n",
    "                               CFG.layerwise_lr,CFG.layerwise_weight_decay,CFG.layerwise_lr_decay)\n",
    "    optimizer = AdamW(grouped_optimizer_params,lr = CFG.layerwise_lr,eps = CFG.layerwise_adam_epsilon)\n",
    "       \n",
    "\n",
    "    num_train_steps = len(train_loader) * CFG.epochs\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "    best_score = np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs): # 开始训练\n",
    "\n",
    "        start_time = time.time()\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, CFG.device)\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, CFG.device)\n",
    "        \n",
    "        # scoring\n",
    "        score, scores = MCRMSE(valid_labels, predictions)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        logger.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        logger.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            logger.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        CFG.OUTPUT_DIR+f\"_fold{fold}_best.pth\")\n",
    "\n",
    "    predictions = torch.load(CFG.OUTPUT_DIR+f\"_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds # 返回验证集，方便后续看4折的验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df[CFG.target_cols].values\n",
    "        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "        score, scores = MCRMSE(labels, preds)\n",
    "        logger.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "    \n",
    "    oof_df = pd.DataFrame()\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold in CFG.trn_fold:\n",
    "            _oof_df = train_loop(df, fold)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            get_result(_oof_df)\n",
    "    oof_df = oof_df.reset_index(drop=True)\n",
    "    logger.info(f\"========== CV ==========\")\n",
    "    get_result(oof_df)\n",
    "    oof_df.to_pickle(CFG.OUTPUT_DIR+'oof_df.pkl')\n",
    "        \n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========== fold: 0 training ==========\n",
    "Epoch 1 - avg_train_loss: 0.1693  avg_val_loss: 0.1240  time: 488s\n",
    "Epoch 1 - Score: 0.4990  Scores: [0.5068011764164785, 0.4791879849875024, 0.47332539798839934, 0.4852436967113309, 0.5328100636640271, 0.5169143823731841]\n",
    "Epoch 1 - Save Best Score: 0.4990 Model\n",
    "Epoch 2 - avg_train_loss: 0.1063  avg_val_loss: 0.1058  time: 487s\n",
    "Epoch 2 - Score: 0.4609  Scores: [0.48620668731814176, 0.450165773830283, 0.44256680585421526, 0.4585972632327725, 0.47338051475309717, 0.4544070761349779]\n",
    "Epoch 2 - Save Best Score: 0.4609 Model\n",
    "Epoch 3 - avg_train_loss: 0.0901  avg_val_loss: 0.1004  time: 487s\n",
    "Epoch 3 - Score: 0.4484  Scores: [0.47639982021778765, 0.44438544047031964, 0.411117580524018, 0.4569625026301624, 0.464023683765807, 0.4375083818812937]\n",
    "Epoch 3 - Save Best Score: 0.4484 Model\n",
    "Epoch 4 - avg_train_loss: 0.0769  avg_val_loss: 0.1018  time: 487s\n",
    "Epoch 4 - Score: 0.4515  Scores: [0.47984098200157826, 0.44881766399298806, 0.4130615578350004, 0.4576644457032279, 0.4677475640102133, 0.4418358456261048]\n",
    "Epoch 5 - avg_train_loss: 0.0672  avg_val_loss: 0.1016  time: 487s\n",
    "Epoch 5 - Score: 0.4512  Scores: [0.47927416519542376, 0.44738998129598806, 0.4093258101051325, 0.45951606347882984, 0.4696585600303837, 0.44177109396991626]\n",
    "Score: 0.4484  Scores: [0.47639982021778765, 0.44438544047031964, 0.411117580524018, 0.4569625026301624, 0.464023683765807, 0.4375083818812937]\n",
    "========== fold: 1 training ==========\n",
    "Epoch 1 - avg_train_loss: 0.1662  avg_val_loss: 0.1204  time: 487s\n",
    "Epoch 1 - Score: 0.4925  Scores: [0.5056467448784349, 0.4648787544504544, 0.4967273213577222, 0.4750627279245587, 0.49216359645916546, 0.5206471385726051]\n",
    "Epoch 1 - Save Best Score: 0.4925 Model\n",
    "Epoch 2 - avg_train_loss: 0.0997  avg_val_loss: 0.1056  time: 488s\n",
    "Epoch 2 - Score: 0.4605  Scores: [0.4971003123194118, 0.4500341659005146, 0.4257122999776533, 0.45545381666135737, 0.4822106513936415, 0.45268714868147086]\n",
    "Epoch 2 - Save Best Score: 0.4605 Model\n",
    "Epoch 3 - avg_train_loss: 0.0833  avg_val_loss: 0.1059  time: 488s\n",
    "Epoch 3 - Score: 0.4612  Scores: [0.4936957684172258, 0.45092886644820057, 0.42686688565018177, 0.4580027652809908, 0.47724159517486314, 0.4604922523197813]\n",
    "Epoch 4 - avg_train_loss: 0.0663  avg_val_loss: 0.1049  time: 487s\n",
    "Epoch 4 - Score: 0.4590  Scores: [0.4859219760592047, 0.45230871285908897, 0.425312712857075, 0.45792414290173, 0.4809722936606625, 0.4512625853735805]\n",
    "Epoch 4 - Save Best Score: 0.4590 Model\n",
    "Epoch 5 - avg_train_loss: 0.0566  avg_val_loss: 0.1049  time: 487s\n",
    "Epoch 5 - Score: 0.4589  Scores: [0.4893536652962534, 0.4516149562135857, 0.42116027137885914, 0.4559525101568498, 0.48145626304991035, 0.4536839864791965]\n",
    "Epoch 5 - Save Best Score: 0.4589 Model\n",
    "Score: 0.4589  Scores: [0.4893536652962534, 0.4516149562135857, 0.42116027137885914, 0.4559525101568498, 0.48145626304991035, 0.4536839864791965]\n",
    "========== fold: 2 training ==========\n",
    "Epoch 1 - avg_train_loss: 0.1723  avg_val_loss: 0.1130  time: 487s\n",
    "Epoch 1 - Score: 0.4763  Scores: [0.5205435724871297, 0.468377265963482, 0.43295119984875896, 0.47738764312971005, 0.4866491874854989, 0.47171757768038786]\n",
    "Epoch 1 - Save Best Score: 0.4763 Model\n",
    "Epoch 2 - avg_train_loss: 0.1005  avg_val_loss: 0.1175  time: 487s\n",
    "Epoch 2 - Score: 0.4860  Scores: [0.5021372958293842, 0.4899600435816936, 0.424252956553942, 0.4772738060423237, 0.5081447589448899, 0.5139416663452717]\n",
    "Epoch 3 - avg_train_loss: 0.0860  avg_val_loss: 0.1108  time: 487s\n",
    "Epoch 3 - Score: 0.4722  Scores: [0.49568937066705004, 0.4519210785216792, 0.45507177069803945, 0.4681841184861647, 0.4930363580993783, 0.4690880836202353]\n",
    "Epoch 3 - Save Best Score: 0.4722 Model\n",
    "Epoch 4 - avg_train_loss: 0.0709  avg_val_loss: 0.1072  time: 487s\n",
    "Epoch 4 - Score: 0.4642  Scores: [0.48999239936988076, 0.45163860143314105, 0.4258305501290957, 0.47141093036783194, 0.4889273032718766, 0.4573709682302424]\n",
    "Epoch 4 - Save Best Score: 0.4642 Model\n",
    "Epoch 5 - avg_train_loss: 0.0627  avg_val_loss: 0.1065  time: 488s\n",
    "Epoch 5 - Score: 0.4627  Scores: [0.4877636587908424, 0.45136349987020213, 0.4240382780997242, 0.4713320188777592, 0.4850065780075501, 0.4567415286553652]\n",
    "Epoch 5 - Save Best Score: 0.4627 Model\n",
    "Score: 0.4627  Scores: [0.4877636587908424, 0.45136349987020213, 0.4240382780997242, 0.4713320188777592, 0.4850065780075501, 0.4567415286553652]\n",
    "========== fold: 3 training ==========\n",
    "Epoch 1 - avg_train_loss: 0.1844  avg_val_loss: 0.1151  time: 487s\n",
    "Epoch 1 - Score: 0.4805  Scores: [0.5102071933743988, 0.4506027609082609, 0.44414139842761974, 0.45583962150504503, 0.5281028548525518, 0.4942996703947343]\n",
    "Epoch 1 - Save Best Score: 0.4805 Model\n",
    "Epoch 2 - avg_train_loss: 0.1091  avg_val_loss: 0.1188  time: 487s\n",
    "Epoch 2 - Score: 0.4888  Scores: [0.5210994618374687, 0.4858118375516794, 0.4614301154196183, 0.47067098617085135, 0.518738699069433, 0.4753169691247772]\n",
    "Epoch 3 - avg_train_loss: 0.0966  avg_val_loss: 0.1049  time: 488s\n",
    "Epoch 3 - Score: 0.4588  Scores: [0.49123557849409555, 0.45253125190846955, 0.41662587405688994, 0.4608782508416372, 0.4896129863687229, 0.44217075418714963]\n",
    "Epoch 3 - Save Best Score: 0.4588 Model\n",
    "Epoch 4 - avg_train_loss: 0.0827  avg_val_loss: 0.1029  time: 488s\n",
    "Epoch 4 - Score: 0.4542  Scores: [0.4881365872323962, 0.4468931987751889, 0.41357093969312186, 0.45561875959214704, 0.4797292994146053, 0.44149608047346883]\n",
    "Epoch 4 - Save Best Score: 0.4542 Model\n",
    "Epoch 5 - avg_train_loss: 0.0740  avg_val_loss: 0.1002  time: 487s\n",
    "Epoch 5 - Score: 0.4481  Scores: [0.4869053142647347, 0.44166370456940907, 0.41296169651420267, 0.44225207596395494, 0.47042158620227514, 0.43460859072352703]\n",
    "Epoch 5 - Save Best Score: 0.4481 Model\n",
    "Score: 0.4481  Scores: [0.4869053142647347, 0.44166370456940907, 0.41296169651420267, 0.44225207596395494, 0.47042158620227514, 0.43460859072352703]\n",
    "========== CV ==========\n",
    "Score: 0.4546  Scores: [0.4851313644810512, 0.4472768544548916, 0.41735362690386074, 0.45674088025058435, 0.47529988932109074, 0.44573896179506994]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs,label in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('C:/Users/86134/Desktop/sample_submission.csv')\n",
    "\n",
    "predictions = []\n",
    "model = FB3Model(CFG, config_path = './config.pth',pretrained=False)\n",
    "model.load_state_dict(torch.load(CFG.OUTPUT_DIR + \"_best.pth\",map_location=torch.device('cpu'))['model'])\n",
    "\n",
    "prediction = inference_fn(test_loader, model, CFG.device)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for fold in CFG.trn_fold:\n",
    "    model = FB3Model(CFG, config_path=CFG.OUTPUT_DIR+'/config.pth', pretrained=False)\n",
    "    state = torch.load(CFG.OUTPUT_DIR +f\"_fold{fold}_best.pth\")\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model, CFG.device)\n",
    "    predictions.append(prediction)\n",
    "    del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "print(predictions)\n",
    "predictions = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(CFG.train_file)\n",
    "test_df = pd.read_csv(CFG.test_file)\n",
    "test_df['labels']=None\n",
    "test_df['labels']=test_df['labels'].apply(lambda x:[0,0,0,0,0,0])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n",
    "test_encoding,test_label=preprocess(test_df,tokenizer,False)\n",
    "test_dataset=MyDataset(test_encoding,test_label)\n",
    "test_loader=DataLoader(test_dataset,batch_size=CFG.batch_size,\n",
    "                       num_workers=CFG.num_workers,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLDR\n",
    "def get_optimizer_grouped_parameters(model, layerwise_lr,layerwise_weight_decay,layerwise_lr_decay):\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    # initialize lr for task specific layer\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "                                     \"weight_decay\": 0.0,\"lr\": layerwise_lr,},]\n",
    "    # initialize lrs for every layer\n",
    "    layers = [model.model.embeddings] + list(model.model.encoder.layer)\n",
    "    layers.reverse()\n",
    "    lr = layerwise_lr\n",
    "    for layer in layers:\n",
    "        optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                                          \"weight_decay\": layerwise_weight_decay,\"lr\": lr,},\n",
    "                                         {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                          \"weight_decay\": 0.0,\"lr\": lr,},]\n",
    "        lr *= layerwise_lr_decay\n",
    "    return optimizer_grouped_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc0f644ff30233460b5696546b915774510ece0528061a54d8886941b056070f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
